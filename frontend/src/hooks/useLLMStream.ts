// hooks/useLLMStream.ts

import { useState, useRef } from "react";

type Message = { role: "user" | "assistant"; content: string };

type APIMessage = { role: "system" | "user" | "assistant"; content: string };

type APIConfig = {
  systemPrompt: string;
  temperature: number;
  maxTokens: number;
  currentModel: string;
  onEmotion?: (emotion: string) => void; // å°‡æƒ…ç·’tokenåŠ å…¥å‹åˆ¥
};

export default function useLLMStream({
  systemPrompt,
  temperature,
  maxTokens,
  currentModel,
  onEmotion,
}: APIConfig) {
  // -----------------------------------------
  // æ ¸å¿ƒèŠå¤©ç‹€æ…‹
  // -----------------------------------------
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState("");
  const [loading, setLoading] = useState(false);
  const [followUpQuestions, setFollowUpQuestions] = useState<string[]>([]);

  // -----------------------------------------
  // SSE ç›¸é—œçš„ Ref
  // -----------------------------------------
  const chunksRef = useRef<string[]>([]);
  const llmBufferRef = useRef<string>("");

  // -----------------------------------------
  // å‡æµå¼å‹•ç•«
  // -----------------------------------------
  function startStreamingDisplay(fullText: string, onFinish?: () => void) {
    let i = 0;
    const delay = 30; // å¯èª¿æ•´

    // æ’å…¥ä¸€æ¢ç©ºçš„ assistant è¨Šæ¯
    setMessages((prev) => [...prev, { role: "assistant", content: "" }]);

    const interval = setInterval(() => {
      i++;
      const partial = fullText.slice(0, i);

      setMessages((prev) => {
        const updated = [...prev];
        const lastIndex = updated.length - 1;
        updated[lastIndex] = {
          ...updated[lastIndex],
          content: partial,
        };
        return updated;
      });

      if (i >= fullText.length) {
        clearInterval(interval);
        if (onFinish) onFinish();
      }
    }, delay);
  }

  // -----------------------------------------
  // è§£æå»¶ä¼¸å•é¡Œ
  // -----------------------------------------
  function parseSuggestions(text: string): string[] {
    try {
      const arr = JSON.parse(text);
      if (Array.isArray(arr)) return arr;
    } catch (_) {}

    const match = text.match(/\[[\s\S]*\]/);
    if (match) {
      try {
        const arr = JSON.parse(match[0]);
        if (Array.isArray(arr)) return arr;
      } catch (_) {}
    }

    return text
      .split(/\n|,|ã€‚/g)
      .map((s) => s.trim())
      .filter(Boolean)
      .slice(0, 3);
  }

  // -----------------------------------------
  // ğŸ”¥ æ ¸å¿ƒï¼šhandleSendï¼ˆå« SSEï¼‰
  // -----------------------------------------
  const handleSend = async (customInput?: string) => {
    const text = customInput ?? input;
    if (!text.trim()) return;

    // å…ˆæ’å…¥ä½¿ç”¨è€…è¨Šæ¯
    const userMsg: Message = { role: "user", content: text };
    setMessages((prev) => [...prev, userMsg]);
    setInput("");
    setLoading(true);
    setFollowUpQuestions([]);

    // çµ„ API messages
    const messagesForAPI: APIMessage[] = [
      { role: "system", content: systemPrompt },
      ...messages.map(
        (m) => ({ role: m.role, content: m.content } as APIMessage)
      ),
      userMsg,
    ];

    try {
      // é–‹ SSE è«‹æ±‚
      const response = await fetch("http://localhost:8000/v2/react/stream", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          messages: messagesForAPI,
          llm_config: {
            model: currentModel,
            temperature,
            max_tokens: maxTokens,
          },
          thread_id: "thread-frontend",
        }),
      });

      if (!response.body) throw new Error("å¾Œç«¯æ²’æœ‰ body");

      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let buffer = "";

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });
        const parts = buffer.split("\n\n");
        buffer = parts.pop() || "";

        for (const part of parts) {
          if (!part.trim()) continue;

          const lines = part.split("\n");
          const eventLine = lines.find((l) => l.startsWith("event:"));
          const dataLine = lines.find((l) => l.startsWith("data:"));
          if (!eventLine || !dataLine) continue;

          const eventType = eventLine
            .replace("event:", "")
            .trim()
            .toLowerCase();
          const data = JSON.parse(dataLine.replace("data:", "").trim());

          // ----------------------------------------------------
          // è™•ç†ä¸åŒäº‹ä»¶
          // ----------------------------------------------------
          switch (eventType) {
            case "tool_chosen":
              setMessages((prev) => [
                ...prev,
                {
                  role: "assistant",
                  content: `ä½¿ç”¨å·¥å…·ï¼š${data.used_tools.name}`,
                },
              ]);
              break;

            case "tool_output":
              console.log("ğŸ”§ å·¥å…·è¼¸å‡ºï¼š", data);
              break;

            case "emotion":
              if (onEmotion && data.emotion) {
                onEmotion(data.emotion);
              }
              break;

            case "llm_start":
              chunksRef.current = [];
              if (data.message_chunk)
                chunksRef.current.push(data.message_chunk);
              break;

            case "llm_delta":
              if (data.message_chunk)
                chunksRef.current.push(data.message_chunk);
              break;

            case "llm_end":
              llmBufferRef.current = chunksRef.current.join("");

              startStreamingDisplay(llmBufferRef.current, async () => {
                // è¦æ±‚å»¶ä¼¸å•é¡Œ
                try {
                  const res = await fetch(
                    "http://localhost:8000/v2/react/run",
                    {
                      method: "POST",
                      headers: { "Content-Type": "application/json" },
                      body: JSON.stringify({
                        messages: [
                          {
                            role: "system",
                            content:
                              "ä½ æ˜¯ä¸€å€‹åŠ©æ‰‹ã€‚åªå…è¨±è¼¸å‡º JSON é™£åˆ—ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚",
                          },
                          {
                            role: "user",
                            content:
                              'è«‹æ ¹æ“šä»¥ä¸‹å›ç­”ç”Ÿæˆä¸‰å€‹å»¶ä¼¸è¿½å•å•é¡Œï¼Œè¼¸å‡ºæ ¼å¼å¿…é ˆæ˜¯ JSON é™£åˆ—ã€‚ä¾‹å¦‚:["å•é¡Œ1","å•é¡Œ2","å•é¡Œ3"]ã€‚\n\nå›ç­”å…§å®¹: ' +
                              llmBufferRef.current,
                          },
                        ],
                        llm_config: {
                          model: currentModel,
                          temperature,
                          max_tokens: 128,
                        },
                        thread_id: "thread-suggestions",
                      }),
                    }
                  );

                  const json = await res.json();
                  const text = json?.messages?.[0]?.content ?? "";
                  setFollowUpQuestions(parseSuggestions(text));
                } catch (err) {
                  console.error("å»¶ä¼¸å•é¡ŒéŒ¯èª¤ï¼š", err);
                }
              });

              setLoading(false);
              break;

            default:
              console.warn("æœªçŸ¥äº‹ä»¶ï¼š", eventType, data);
          }
        }
      }

      setLoading(false);
    } catch (err) {
      console.error("handleSend éŒ¯èª¤ï¼š", err);
      setMessages((prev) => [
        ...prev,
        { role: "assistant", content: "ç³»çµ±éŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦" },
      ]);
      setLoading(false);
    }
  };

  // -----------------------------------------
  // Hook å°å¤–æä¾›çš„å…§å®¹
  // -----------------------------------------
  return {
    messages,
    followUpQuestions,
    input,
    setInput,
    loading,
    handleSend,
  };
}
